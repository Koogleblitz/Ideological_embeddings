{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the expanded corpus from the file\n",
    "Each word is parsed. We can update this however we like as long as it is some sort of text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"ideological_corpus.txt\", \"r\") as f:\n",
    "    corpus = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Number of topics (K) we want to identify\n",
    "K = 3  # Military, Rights, Economics\n",
    "\n",
    "# Vectorize the corpus and fit LDA to identify topics\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "lda = LatentDirichletAllocation(n_components=K, random_state=0)\n",
    "X_topics = lda.fit_transform(X)\n",
    "topic_words = lda.components_\n",
    "\n",
    "# Display the topic words for each topic\n",
    "print(\"Topic words per topic:\")\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    topic_words_list = [vectorizer.get_feature_names_out()[j] for j in topic_dist.argsort()[:-10 - 1:-1]]\n",
    "    print(f\"Topic {i}: {', '.join(topic_words_list)}\")\n",
    "\n",
    "V = 30  # Number of users\n",
    "D = [np.random.choice(range(V), size=np.random.randint(1, V), replace=False) for _ in range(len(corpus))]\n",
    "\n",
    "# Simulated user network (adjacency matrix) - V users\n",
    "E = np.random.randint(0, 2, (V, V))\n",
    "\n",
    "# Initialize ùúô (polarities) and ùúÉ (interests) as |V| x K matrices\n",
    "phi = np.random.rand(V, K)\n",
    "theta = np.random.rand(V, K)\n",
    "\n",
    "# Number of epochs which is the number of times it runs for training\n",
    "# Learning rate its rate to which it will learn\n",
    "epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "The model iteratively updates the phi and theta matrices using a gradient ascent approach\n",
    "* Alignment Probability: The function alignment_probability calculates the probability that two users align ideologically on a specific topic, based on their phi values. \n",
    "* Updating Matrices: For each epoch, the model updates the phi and theta matrices based on observed user-document interactions and negative sampling (to account for\n",
    "unobserved interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute p(u, v, k) which indicates the alignment probability\n",
    "def alignment_probability(u, v, k):\n",
    "    return phi[u, k] * phi[v, k] + (1 - phi[u, k]) * (1 - phi[v, k])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(corpus)):\n",
    "        for v in D[i]:\n",
    "            for u in np.where(E[v] == 1)[0]:  # Users u that follow v\n",
    "                if u in D[i]:\n",
    "                    # Update phi, theta for observed activations\n",
    "                    for k in range(K):\n",
    "                        gradient_phi = X_topics[i, k] * theta[u, k] * (phi[v, k] - phi[u, k])\n",
    "                        gradient_theta = X_topics[i, k] * alignment_probability(u, v, k)\n",
    "                        phi[u, k] += learning_rate * gradient_phi\n",
    "                        theta[u, k] += learning_rate * gradient_theta\n",
    "\n",
    "            # Negative sampling for unobserved activations\n",
    "            unobserved_users = np.setdiff1d(range(V), D[i])\n",
    "            sample_size = min(len(unobserved_users), len(D[i]))\n",
    "            sampled_unobserved_users = np.random.choice(unobserved_users, size=sample_size, replace=False)\n",
    "            for u in sampled_unobserved_users:\n",
    "                if E[v, u] == 1:\n",
    "                    for k in range(K):\n",
    "                        gradient_phi = X_topics[i, k] * theta[u, k] * (phi[v, k] - phi[u, k])\n",
    "                        gradient_theta = X_topics[i, k] * alignment_probability(u, v, k)\n",
    "                        phi[u, k] -= learning_rate * gradient_phi\n",
    "                        theta[u, k] -= learning_rate * gradient_theta\n",
    "\n",
    "# Output the resulting phi and theta matrices\n",
    "print(\"\\nPolarities (phi):\")\n",
    "print(phi)\n",
    "print(\"\\nInterests (theta):\")\n",
    "print(theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Interpretation:\n",
    "The final step involves visualizing the ideological embeddings. Users are plotted in a 2D space using the first two dimensions of the phi matrix. Each point\n",
    "represents a user, positioned according to their ideological stance. Interpreting Positions: The visualization\n",
    "helps in understanding the relative ideological positions of users, with closer points indicating similar\n",
    "ideologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the embeddings\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, user_embedding in enumerate(phi):\n",
    "    plt.scatter(user_embedding[0], user_embedding[1], label=f'User {i + 1}')\n",
    "    plt.text(user_embedding[0], user_embedding[1], f'User {i + 1}', fontsize=9)\n",
    "\n",
    "\n",
    "plt.xlabel('Ideological Dimension 1')\n",
    "plt.ylabel('Ideological Dimension 2')\n",
    "plt.title('Ideological Embeddings with Expanded Corpus')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
